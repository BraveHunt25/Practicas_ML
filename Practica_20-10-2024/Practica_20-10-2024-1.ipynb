{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 5: medidas de desempeño en ML\n",
    "## Integrantes:\n",
    "- Hernández Jiménez Erick Yael\n",
    "- Patiño Flores Samuel\n",
    "- Robert Garayzar Arturo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1\n",
    "Programa, sin bibliotecas, las funciones para calcular las siguientes medidas de desempeño:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_accuracy(TP, TN, FP, FN):\n",
    "    return (TP + TN) / (TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_error(TP, TN, FP, FN):\n",
    "    return 1 - calcular_accuracy(TP, TN, FP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_precision(TP, FP):\n",
    "    if (TP + FP) == 0:\n",
    "        return 0\n",
    "    return TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_recall(TP, FN):\n",
    "    if (TP + FN) == 0:\n",
    "        return 0\n",
    "    return TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_true_positive_rate(TP, FN):\n",
    "    return calcular_recall(TP, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_true_negative_rate(TN, FP):\n",
    "    if (TN + FP) == 0:\n",
    "        return 0\n",
    "    return TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_false_positive_rate(FP, TN):\n",
    "    if (FP + TN) == 0:\n",
    "        return 0\n",
    "    return FP / (FP + TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_false_negative_rate(FN, TP):\n",
    "    if (FN + TP) == 0:\n",
    "        return 0\n",
    "    return FN / (FN + TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_f1_score(TP, FP, FN):\n",
    "    precision = calcular_precision(TP, FP)\n",
    "    recall = calcular_recall(TP, FN)\n",
    "    if (precision + recall) == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la importación del archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def leer_csv(archivo_csv):\n",
    "    verdaderos = []\n",
    "    predicciones = []\n",
    "    with open(archivo_csv, newline='') as archivo:\n",
    "        lector = csv.DictReader(archivo)\n",
    "        for fila in lector:\n",
    "            verdaderos.append(int(fila['verdaderos']))\n",
    "            predicciones.append(int(fila['predicciones']))\n",
    "    return verdaderos, predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el cálculo de matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_matriz_confusion(verdaderos, predicciones):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for v, p in zip(verdaderos, predicciones):\n",
    "        if v == 1 and p == 1:\n",
    "            TP += 1\n",
    "        elif v == 0 and p == 0:\n",
    "            TN += 1\n",
    "        elif v == 0 and p == 1:\n",
    "            FP += 1\n",
    "        elif v == 1 and p == 0:\n",
    "            FN += 1\n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la función que calcula todas las métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_metricas(archivo_csv):\n",
    "    verdaderos, predicciones = leer_csv(archivo_csv)\n",
    "    TP, TN, FP, FN = calcular_matriz_confusion(verdaderos, predicciones)\n",
    "    \n",
    "    accuracy = calcular_accuracy(TP, TN, FP, FN)\n",
    "    error = calcular_error(TP, TN, FP, FN)\n",
    "    precision = calcular_precision(TP, FP)\n",
    "    recall = calcular_recall(TP, FN)\n",
    "    tpr = calcular_true_positive_rate(TP, FN)\n",
    "    tnr = calcular_true_negative_rate(TN, FP)\n",
    "    fpr = calcular_false_positive_rate(FP, TN)\n",
    "    fnr = calcular_false_negative_rate(FN, TP)\n",
    "    f1_score = calcular_f1_score(TP, FP, FN)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Error: {error}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"True Positive Rate (TPR): {tpr}\")\n",
    "    print(f\"True Negative Rate (TNR): {tnr}\")\n",
    "    print(f\"False Positive Rate (FPR): {fpr}\")\n",
    "    print(f\"False Negative Rate (FNR): {fnr}\")\n",
    "    print(f\"F1-Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usó el ejemplo en clase y se redujo la predicción de 3 clases a 2:\n",
    "- 1: se predijo que la clase era 1\n",
    "- 0: se predijo que la clase era distinta a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "Error: 0.33333333333333337\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "True Positive Rate (TPR): 1.0\n",
      "True Negative Rate (TNR): 0.5\n",
      "False Positive Rate (FPR): 0.5\n",
      "False Negative Rate (FNR): 0.0\n",
      "F1-Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "archivo_csv = './dataset/Ejercicio_C_Euclidiano.csv'\n",
    "calcular_metricas(archivo_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2\n",
    "Investiga, y en su caso ejemplifica, si las medidas descritas anteriormente están disponibles en alguna biblioteca de Python y proporciona un ejemplo.\n",
    "> Las medidas de rendimiento descritas anteriormente (Accuracy, Precision, Recall, F1-Score, etc.) están disponibles en varias bibliotecas de Python, siendo la más popular scikit-learn. Esta biblioteca proporciona funciones listas para usar que permiten calcular estas métricas de manera sencilla.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos los mismos datos del csv original del ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "verdaderos = [3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1]\n",
    "predicciones = [1, 3, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1]\n",
    "\n",
    "verdaderos_binarios = [1 if v == 1 else 0 for v in verdaderos]\n",
    "predicciones_binarias = [1 if p == 1 else 0 for p in predicciones]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la matriz de confusión y aplicamos las métricas sobre los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[5 5]\n",
      " [0 5]]\n",
      "Accuracy: 0.6666666666666666\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "F1-Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Calcular la matriz de confusión\n",
    "matriz_confusion = confusion_matrix(verdaderos_binarios, predicciones_binarias)\n",
    "\n",
    "# Calcular métricas de rendimiento\n",
    "accuracy = accuracy_score(verdaderos_binarios, predicciones_binarias)\n",
    "precision = precision_score(verdaderos_binarios, predicciones_binarias)\n",
    "recall = recall_score(verdaderos_binarios, predicciones_binarias)\n",
    "f1 = f1_score(verdaderos_binarios, predicciones_binarias)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Matriz de confusión:\")\n",
    "print(matriz_confusion)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
